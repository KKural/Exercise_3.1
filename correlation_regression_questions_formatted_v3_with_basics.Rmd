---
title: "Correlation & Regression ‚Äî Enhanced Microcourse with Visuals (MCQ format)"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(ggplot2)
library(flextable)
library(dplyr)
```

# Correlation and Regression ‚Äî Enhanced with Visual Learning

This comprehensive assessment covers correlation and regression analysis concepts essential for criminology statistics. The file includes 80 carefully structured questions across five modules, with visual examples, practical applications, and detailed feedback for optimal learning.

## Visual Learning Guide

### Understanding Correlation Patterns Through Scatterplots

```{r correlation-patterns, echo=TRUE, fig.width=12, fig.height=8}
# Generate example data for different correlation patterns
set.seed(123)
n <- 50

# Strong positive correlation (r ‚âà 0.85)
x1 <- rnorm(n, 50, 10)
y1 <- 0.85 * scale(x1)[,1] * 8 + rnorm(n, 60, 3)

# Strong negative correlation (r ‚âà -0.75)
x2 <- rnorm(n, 50, 10)
y2 <- -0.75 * scale(x2)[,1] * 8 + rnorm(n, 60, 4)

# Weak correlation (r ‚âà 0.15)
x3 <- rnorm(n, 50, 10)
y3 <- 0.15 * scale(x3)[,1] * 8 + rnorm(n, 60, 8)

# No correlation (r ‚âà 0)
x4 <- rnorm(n, 50, 10)
y4 <- rnorm(n, 60, 8)

# Create combined dataset
data_patterns <- data.frame(
  x = c(x1, x2, x3, x4),
  y = c(y1, y2, y3, y4),
  pattern = rep(c("Strong Positive (r ‚âà +0.85)", "Strong Negative (r ‚âà -0.75)", 
                  "Weak Positive (r ‚âà +0.15)", "No Correlation (r ‚âà 0)"), each = n)
)

# Create the visualization
ggplot(data_patterns, aes(x = x, y = y)) +
  geom_point(alpha = 0.7, size = 2, color = "steelblue") +
  geom_smooth(method = "lm", se = TRUE, color = "red", linewidth = 1) +
  facet_wrap(~pattern, ncol = 2, scales = "free") +
  labs(
    title = "Understanding Correlation Patterns in Criminology Data",
    subtitle = "How data points cluster around the trend line indicates correlation strength",
    x = "Predictor Variable (e.g., socioeconomic status)",
    y = "Outcome Variable (e.g., crime rate)",
    caption = "Note: Shaded area shows confidence interval around regression line"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 11),
    strip.text = element_text(size = 10, face = "bold"),
    axis.title = element_text(size = 10),
    plot.caption = element_text(size = 9, style = "italic")
  )
```

**Key Interpretation Points:**
- **Strong Positive**: Points cluster tightly around upward-sloping line
- **Strong Negative**: Points cluster tightly around downward-sloping line  
- **Weak Correlation**: Points scattered widely around trend line
- **No Correlation**: No discernible linear pattern, flat or chaotic arrangement

### Regression Line Components

```{r regression-components, echo=TRUE, fig.width=10, fig.height=6}
# Create example regression data
set.seed(456)
x_reg <- seq(10, 50, length.out = 30)
y_reg <- 15 + 0.8 * x_reg + rnorm(30, 0, 4)

# Calculate regression line
model <- lm(y_reg ~ x_reg)
intercept <- coef(model)[1]
slope <- coef(model)[2]

# Create regression visualization
ggplot(data.frame(x = x_reg, y = y_reg), aes(x = x, y = y)) +
  geom_point(size = 3, alpha = 0.7, color = "darkblue") +
  geom_smooth(method = "lm", se = FALSE, color = "red", linewidth = 1.5) +
  # Highlight intercept
  geom_point(aes(x = 0, y = intercept), color = "green", size = 4, shape = 17) +
  # Add annotations
  annotate("text", x = 5, y = intercept + 3, 
           label = paste("Y-intercept =", round(intercept, 1)), 
           color = "green", size = 4, fontface = "bold") +
  annotate("text", x = 35, y = 25, 
           label = paste("Slope =", round(slope, 2)), 
           color = "red", size = 4, fontface = "bold") +
  # Add residual examples
  geom_segment(aes(x = x_reg[5], y = y_reg[5], 
                   xend = x_reg[5], yend = predict(model)[5]), 
               color = "orange", linewidth = 1, linetype = "dashed") +
  annotate("text", x = x_reg[5] + 2, y = (y_reg[5] + predict(model)[5])/2, 
           label = "Residual", color = "orange", size = 3, fontface = "bold") +
  labs(
    title = "Anatomy of a Regression Line",
    subtitle = "Understanding slope, intercept, and residuals in criminology regression",
    x = "Predictor Variable (X)",
    y = "Outcome Variable (Y)",
    caption = "Green triangle = Y-intercept | Red line = Regression line | Orange dashed = Residual (error)"
  ) +
  xlim(0, 55) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 11),
    axis.title = element_text(size = 10),
    plot.caption = element_text(size = 9, style = "italic")
  )
```

**Component Explanations:**
- **Y-intercept**: Value of Y when X = 0 (may not be meaningful in all contexts)
- **Slope**: Change in Y for each 1-unit increase in X
- **Residuals**: Vertical distance between observed points and predicted line
- **R¬≤**: Proportion of variance in Y explained by X (closer to 1.0 = better fit)

### Understanding Standard Error in Regression

**Standard Error (SE)** measures the precision of our regression estimates:

- **SE of slope**: How much the slope might vary across different samples
- **Smaller SE**: More precise estimate, narrow confidence intervals
- **Larger SE**: Less precise estimate, wide confidence intervals
- **95% CI**: Slope ¬± (1.96 √ó SE) gives the range we're 95% confident contains the true population slope

In criminology: A small SE for the relationship between unemployment and crime suggests this relationship is precisely estimated and likely reliable across different communities.

## APA-Style Regression Results Table

```{r apa-table, echo=TRUE}
# Create example regression results for criminology research
regression_results <- data.frame(
  Predictor = c("(Intercept)", "Unemployment Rate", "Population Density", "Education Level"),
  B = c(45.23, 2.14, 0.003, -1.87),
  SE = c(8.91, 0.45, 0.001, 0.62),
  t = c(5.08, 4.76, 3.00, -3.02),
  p = c("< .001", "< .001", ".003", ".003"),
  CI_Lower = c(27.75, 1.26, 0.001, -3.08),
  CI_Upper = c(62.71, 3.02, 0.005, -0.66)
)

# Create APA-style table using flextable
apa_table <- flextable(regression_results) %>%
  set_header_labels(
    Predictor = "Predictor",
    B = "B",
    SE = "SE", 
    t = "t",
    p = "p",
    CI_Lower = "95% CI Lower",
    CI_Upper = "95% CI Upper"
  ) %>%
  add_header_lines("Table 1. Regression Analysis Predicting Crime Rate per 1,000 Residents") %>%
  align(align = "center", part = "all") %>%
  align(j = 1, align = "left", part = "body") %>%
  fontsize(size = 10, part = "all") %>%
  fontsize(size = 12, part = "header") %>%
  bold(part = "header") %>%
  autofit() %>%
  add_footer_lines(c(
    "Note. N = 150 cities. R¬≤ = .67, F(3, 146) = 98.45, p < .001.",
    "B = unstandardized regression coefficient; SE = standard error; CI = confidence interval.",
    "All continuous predictors were centered before analysis."
  )) %>%
  fontsize(size = 9, part = "footer") %>%
  italic(part = "footer")

apa_table
```

## Interpretation Guide for Criminology Research

### Effect Size Guidelines (Cohen's Conventions)
- **Small correlation**: r = .10 (1% of variance explained)
- **Medium correlation**: r = .30 (9% of variance explained)  
- **Large correlation**: r = .50 (25% of variance explained)

### Practical Significance vs Statistical Significance
- **Statistical significance**: p < .05 (relationship unlikely due to chance)
- **Practical significance**: Effect size large enough to matter in real-world applications
- **Example**: r = .15, p < .001 in large sample may be statistically significant but practically small

### Common Interpretation Pitfalls
1. **Correlation ‚â† Causation**: High correlation doesn't prove one variable causes another
2. **Third variables**: Alternative explanations may account for observed relationships
3. **Restriction of range**: Limited variability can artificially reduce correlations
4. **Outliers**: Extreme scores can inflate or deflate correlation coefficients
5. **Non-linear relationships**: Correlation assumes linear associations

Note: This file reformats questions from `Complete_Course_Questions.Rmd` to match the MCQ style used in `All quesion/basisbegrippen_statistiek.Rmd`. For items that normally require visuals, the phrasing is conceptual.

## OVERVIEW (Expanded course with fundamentals ‚Äî 86 questions)

### Module 1: Correlation Basics (34 questions)
| Question | Title | Level |
|----------|-------|-------|
| Q1 | What is correlation? | Remember |
| Q2 | What is regression? | Remember |
| Q3 | Variable types for correlation | Understand |
| Q4 | Correlation vs causation basics | Understand |
| Q5 | What is a z-score? | Remember |
| Q6 | Interpreting correlation values | Understand |
| Q7 | Key measures in correlation | Remember |
| Q8 | Direction of relationships | Understand |
| Q9 | What does correlation tell us? | Understand |
| Q10 | Correlation vs regression differences | Apply |
| Q11 | Regression slope meaning | Understand |
| Q12 | Regression intercept meaning | Remember |
| Q13 | R¬≤ interpretation | Understand |
| Q14 | Why square residuals? | Analyze |
| Q15 | Correlation-regression connection | Understand |
| Q16 | Zero correlation and slope | Apply |
| Q17 | Correlation and causation | Evaluate |
| Q18 | Why standardize? | Understand |
| Q19 | Center in z‚Äëscores | Remember |
| Q20 | Anscombe's Quartet | Understand |
| Q21 | Correlation ‚â† Causation | Analyze |
| Q22 | Monotone vs. linear | Apply |
| Q23 | Units don't change r | Remember |
| Q24 | Z‚Äëscores compare axes | Understand |
| Q25 | Subgroups vs. overall r | Analyze |
| Q26 | Quadrants and sign | Understand |
| Q27 | Direction and strength | Apply |
| Q28 | Outlier impact | Analyze |
| Q29 | Covariance vs correlation | Understand |
| Q30 | Weak positive | Apply |
| Q31 | Describe the pattern | Understand |
| Q32 | Meaning of z = +1.2 | Remember |
| Q33 | Aggregation pitfall | Analyze |
| Q34 | Bridge to calculations | Apply |

### Module 2: Correlation Calculations (18 questions)
| Question | Title | Level |
|----------|-------|-------|
| Q35 | Compute means | Apply |
| Q36 | Calculate deviations | Apply |
| Q37 | Sum of squares | Apply |
| Q38 | Sample variances | Apply |
| Q39 | Covariance calculation | Apply |
| Q40 | Pearson r (standardized cov) | Apply |
| Q41 | Pearson r (z‚Äëscores) | Understand |
| Q42 | Interpret r magnitude | Analyze |
| Q43 | Compute R¬≤ | Apply |
| Q44 | Spearman from ranks | Apply |
| Q45 | Handling ties in ranks | Understand |
| Q46 | Cram√©r's V calculation | Apply |
| Q47 | Point‚Äëbiserial correlation | Apply |
| Q48 | Choose correlation measure | Analyze |
| Q49 | Outlier sensitivity | Analyze |
| Q50 | When to use Spearman | Evaluate |
| Q51 | Correlation vs covariance | Understand |
| Q52 | Interpretation practice | Apply |

### Module 3a: From Correlation to Regression (7 questions)
| Question | Title | Level |
|----------|-------|-------|
| Q53 | Visual correlation patterns | Understand |
| Q54 | Correlation strength to prediction | Apply |
| Q55 | From r to regression line | Apply |
| Q56 | Prediction accuracy and R¬≤ | Analyze |
| Q57 | Slope interpretation practice | Apply |
| Q58 | Residual patterns | Understand |
| Q59 | When regression is appropriate | Evaluate |

### Module 3: Regression Basics (10 questions)
| Question | Title | Level |
|----------|-------|-------|
| Q60 | OLS principle | Understand |
| Q61 | Line passes through means | Remember |
| Q62 | Slope interpretation | Understand |
| Q63 | Intercept meaning | Understand |
| Q64 | Residuals concept | Understand |
| Q65 | Why squared errors | Understand |
| Q66 | R¬≤ interpretation | Understand |
| Q67 | Extrapolation risks | Analyze |
| Q68 | Correlation vs regression | Understand |
| Q69 | Causality caution | Analyze |

### Module 4: Regression Calculations (12 questions)
| Question | Title | Level |
|----------|-------|-------|
| Q70 | Calculate slope | Apply |
| Q71 | Calculate intercept | Apply |
| Q72 | Make predictions | Apply |
| Q73 | Calculate residuals | Apply |
| Q74 | R¬≤ from correlation | Apply |
| Q75 | Standardized slope | Apply |
| Q76 | Unit effects on regression | Understand |
| Q77 | Read statistical output | Apply |
| Q78 | Outlier effects | Analyze |
| Q79 | Intercept interpretation | Analyze |
| Q80 | Model assumptions | Understand |
| Q81 | Prediction vs explanation | Understand |

### Module 5: Partial Correlation (8 questions)
| Question | Title | Level |
|----------|-------|-------|
| Q73 | When to use partial correlation | Understand |
| Q74 | Partial correlation formula | Remember |
| Q75 | Calculate partial correlation | Apply |
| Q76 | Spurious vs suppression | Analyze |
| Q77 | Control vs interaction | Understand |
| Q78 | Interpret partial correlation | Analyze |
| Q79 | Statistical reporting | Apply |
| Q80 | Scale invariance | Understand |

---

# Module 1: Correlation Basics (24 questions)

### Question Q1 (Remember)
**What is correlation?**

> **Hint:** Think about the relationship between two variables.

1) A measure of how much one variable causes another  
"1" = "‚ùå Incorrect. Correlation does not establish causation."
2) A statistical measure of the strength and direction of relationship between two variables  
"2" = "‚úÖ Correct! Correlation quantifies linear association between variables."
3) The difference between two variables  
"3" = "‚ùå Incorrect. That would be a simple difference, not correlation."
4) A method to predict future values  
"4" = "‚ùå Incorrect. That's prediction; correlation measures association."

---

### Question Q2 (Remember)
**What is regression?**

> **Hint:** Think about predicting one variable from another.

1) The same thing as correlation  
"1" = "‚ùå Incorrect. Regression and correlation are related but different."
2) A statistical method to predict one variable from another using a linear equation  
"2" = "‚úÖ Correct! Regression creates prediction equations like Y = a + bX."
3) A way to calculate averages  
"3" = "‚ùå Incorrect. Regression involves prediction, not just averaging."
4) A method only used for categorical variables  
"4" = "‚ùå Incorrect. Regression typically uses continuous variables."

---

### Question Q3 (Understand)
**For which types of variables can you calculate Pearson correlation?**

> **Hint:** Consider the measurement level requirements.

1) Only categorical variables  
"1" = "‚ùå Incorrect. Pearson correlation requires numeric variables."
2) Both variables must be continuous (interval or ratio level)  
"2" = "‚úÖ Correct! Pearson correlation requires numeric, continuous variables."
3) One categorical and one continuous variable  
"3" = "‚ùå Incorrect. This would use point-biserial correlation."
4) Any type of variables  
"4" = "‚ùå Incorrect. Different correlation types exist for different variable types."

---

### Question Q4 (Understand)
**Why doesn't correlation prove causation?**

> **Hint:** Think about alternative explanations for associations.

1) Correlation is always too weak to prove anything  
"1" = "‚ùå Incorrect. Even strong correlations don't prove causation."
2) Third variables, reverse causation, or coincidence might explain the relationship  
"2" = "‚úÖ Correct! Many alternative explanations exist for statistical associations."
3) Only experiments can show any relationships  
"3" = "‚ùå Incorrect. Correlation shows association; causation requires more evidence."
4) Correlation is the same as causation  
"4" = "‚ùå Incorrect. This is the misconception we want to avoid."

---

### Question Q5 (Remember)
**What is a z-score?**

> **Hint:** Think about standardizing scores relative to the mean and spread.

1) The raw score minus the mean  
"1" = "‚ùå Incorrect. That's a deviation, not a z-score."
2) A score that shows how many standard deviations a value is from the mean  
"2" = "‚úÖ Correct! z = (X - Œº)/œÉ standardizes scores."
3) The percentage above the mean  
"3" = "‚ùå Incorrect. Z-scores use standard deviation units, not percentages."
4) The square of the original score  
"4" = "‚ùå Incorrect. Z-scores involve standardization, not squaring."

---

### Question Q6 (Understand)
**How do you interpret a correlation of r = 0.75?**

> **Hint:** Consider both direction and strength.

1) Weak positive relationship  
"1" = "‚ùå Incorrect. 0.75 is stronger than weak."
2) Strong positive relationship  
"2" = "‚úÖ Correct! Values around 0.7-0.8 indicate strong positive association."
3) Perfect negative relationship  
"3" = "‚ùå Incorrect. The value is positive, not negative or perfect."
4) No relationship  
"4" = "‚ùå Incorrect. 0.75 shows a clear relationship."

---

### Question Q7 (Remember)
**What are the key measures in correlation analysis?**

> **Hint:** Think about the main statistics that describe correlation.

1) Only the correlation coefficient (r)  
"1" = "‚ùå Incorrect. There are additional important measures."
2) Correlation coefficient (r), coefficient of determination (R¬≤), and sample size (n)  
"2" = "‚úÖ Correct! These provide complete information about the relationship."
3) Only the mean and standard deviation  
"3" = "‚ùå Incorrect. These don't describe the relationship between variables."
4) The slope and intercept  
"4" = "‚ùå Incorrect. These are regression parameters, not correlation measures."

---

### Question Q8 (Understand)
**What does the direction of a correlation tell you?**

> **Hint:** Think about positive vs. negative relationships.

1) How strong the relationship is  
"1" = "‚ùå Incorrect. Direction is about sign, not strength."
2) Whether variables increase together (positive) or one increases as the other decreases (negative)  
"2" = "‚úÖ Correct! Direction shows whether variables move in the same or opposite directions."
3) Whether the relationship is causal  
"3" = "‚ùå Incorrect. Direction doesn't indicate causation."
4) How many observations are in the dataset  
"4" = "‚ùå Incorrect. Direction doesn't relate to sample size."

---

### Question Q9 (Understand)
**What does a correlation tell us?**

> **Hint:** Focus on *direction* and *strength* of association.

1) How much one variable causes the other  
"1" = "‚ùå Incorrect. Correlation shows association, not causation."
2) The direction and strength of a linear relationship between two variables  
"2" = "‚úÖ Correct! Correlation quantifies how strongly and in which direction variables move together."
3) The difference between two group means  
"3" = "‚ùå Incorrect. That's a t-test, not correlation."
4) The total variance in one variable  
"4" = "‚ùå Incorrect. Variance describes one variable, not the relationship between two."

---

### Question Q10 (Apply)
**Which statement correctly distinguishes correlation and regression?**

> **Hint:** Think about symmetry vs. prediction.

1) Correlation is asymmetric, regression is symmetric  
"1" = "‚ùå Incorrect. The opposite is true."
2) Correlation measures mutual association; regression predicts Y from X  
"2" = "‚úÖ Correct! Correlation is symmetric; regression is directional and predictive."
3) Regression only applies to categorical data  
"3" = "‚ùå Incorrect. Regression requires numeric predictors."
4) Correlation and regression are identical analyses  
"4" = "‚ùå Incorrect. They are mathematically related but conceptually distinct."

---

### Question Q11 (Understand)
**In a simple regression model, what does the slope (b) represent?**

> **Hint:** Think: how much does Y change when X changes by one unit?

1) The predicted value of Y when X = 0  
"1" = "‚ùå Incorrect. That's the intercept (a)."
2) The amount Y changes for each one-unit increase in X  
"2" = "‚úÖ Correct! The slope shows the expected change in Y for a unit change in X."
3) The variability of Y around its mean  
"3" = "‚ùå Incorrect. That describes variance, not slope."
4) The strength of correlation  
"4" = "‚ùå Incorrect. Slope and correlation are related but not identical."

---

### Question Q12 (Remember)
**What does the intercept (a) represent in regression?**

> **Hint:** It's where the regression line crosses the Y-axis.

> **üîç Detailed Explanation:** The intercept is the predicted value of Y when X equals zero. However, be cautious about interpretation:
> 
> **When intercept is meaningful:**
> - X = 0 is within the observed range of data
> - X = 0 represents a realistic scenario
> - Example: Predicting crime rate from temperature‚Äîintercept is crime rate at 0¬∞C
> 
> **When intercept lacks practical meaning:**
> - X = 0 is far outside the data range (extrapolation)
> - X = 0 is impossible/unrealistic
> - Example: Predicting crime from education years‚Äîintercept assumes 0 years education
> 
> **In criminology contexts:**
> - Predicting recidivism from age: intercept = recidivism rate at age 0 (not meaningful)
> - Predicting crime rate from unemployment rate: intercept = crime rate at 0% unemployment (potentially meaningful)

1) The predicted value of Y when X = 0  
"1" = "‚úÖ Correct! That's exactly the definition, but remember to consider whether X = 0 is meaningful in your context."
2) The strength of the correlation  
"2" = "‚ùå Incorrect. Correlation strength is measured by r, not the intercept."
3) The total error in prediction  
"3" = "‚ùå Incorrect. Residuals represent prediction error; intercept is a model parameter."
4) The average value of Y  
"4" = "‚ùå Incorrect. The intercept equals »≤ only if XÃÑ = 0, which is rarely the case."

---

### Question Q13 (Understand)
**What does R¬≤ tell us in regression?**

> **Hint:** Think about explained variance.

1) The proportion of variance in Y explained by X  
"1" = "‚úÖ Correct! R¬≤ quantifies how much of Y's variation the model explains."
2) The residual variance left unexplained  
"2" = "‚ùå Incorrect. That's (1 ‚àí R¬≤)."
3) The slope of the regression line  
"3" = "‚ùå Incorrect. Slope (b) and R¬≤ are different statistics."
4) The total correlation between all variables  
"4" = "‚ùå Incorrect. R¬≤ is derived from one correlation (r¬≤) in simple regression."

---

### Question Q14 (Analyze)
**Why do we square the residuals when fitting the regression line (OLS method)?**

> **Hint:** Consider the role of positive and negative errors.

1) To make all errors positive and penalize large deviations more heavily  
"1" = "‚úÖ Correct! Squaring ensures positive values and emphasizes larger errors."
2) To simplify the formula for correlation  
"2" = "‚ùå Incorrect. Squaring is for model fitting, not correlation."
3) To reduce the number of data points  
"3" = "‚ùå Incorrect. The sample size remains unchanged."
4) Because residuals are always negative  
"4" = "‚ùå Incorrect. Residuals can be both positive and negative."

---

### Question Q15 (Understand)
**How are correlation and regression mathematically connected?**

> **Hint:** Think about formulas linking r, b, and R¬≤.

1) b = r √ó (Sx / Sy)  
"1" = "‚ùå Incorrect. The ratio is reversed."
2) b = r √ó (Sy / Sx)  
"2" = "‚úÖ Correct! The regression slope equals correlation √ó ratio of standard deviations."
3) r = b √ó (Sy / Sx)  
"3" = "‚ùå Incorrect. That's the inverse relation."
4) R¬≤ = b √ó r  
"4" = "‚ùå Incorrect. R¬≤ = r¬≤, not b √ó r."

---

### Question Q16 (Apply)
**If the correlation between X and Y is zero, what does the regression slope (b) equal?**

> **Hint:** Recall the link between r and b.

1) b = 0  
"1" = "‚úÖ Correct! When r = 0, the regression slope is zero ‚Äî no linear prediction."
2) b = 1  
"2" = "‚ùå Incorrect. That would mean a perfect positive relationship."
3) b = ‚àí1  
"3" = "‚ùå Incorrect. That would mean a perfect negative relationship."
4) b cannot be computed  
"4" = "‚ùå Incorrect. It can be computed, but will be 0."

---

### Question Q17 (Evaluate)
**Which of the following statements about correlation and causation is correct?**

> **Hint:** One term implies direction and mechanism; the other doesn't.

1) Correlation means one variable causes the other  
"1" = "‚ùå Incorrect. Association ‚â† causation."
2) Correlation can suggest a relationship worth investigating, but doesn't prove cause  
"2" = "‚úÖ Correct! Correlation is a clue, not proof of causality."
3) Correlation is only used in experiments  
"3" = "‚ùå Incorrect. Correlation is mainly used in observational research."
4) If r = 1, causation is guaranteed  
"4" = "‚ùå Incorrect. Even a perfect correlation can result from a third variable."

---

### Question Q18 (Understand)
**Why is standardizing (using z‚Äëscores) useful when comparing two variables?**

> **Hint:** Consider unit effects on covariance and correlation as a unit‚Äëfree measure.

1) The relationship fundamentally changes when units change  
"1" = "‚ùå Incorrect. Units don't change the underlying relationship; only covariance's magnitude."
2) The relationship stays the same, but covariance isn't comparable across units  
"2" = "‚úÖ Correct! Correlation (r) is standardized and unit‚Äëfree; covariance isn't."
3) Covariance is always better than correlation  
"3" = "‚ùå Incorrect. Covariance is scale‚Äëdependent; correlation is scale‚Äëinvariant."
4) You should never standardize variables  
"4" = "‚ùå Incorrect. Standardizing is useful to put variables on the same scale."

---

### Question Q10 (Remember)
**Where is the 'center of gravity' in a standardized (z‚Äëscore) scatterplot?**

> **Hint:** Z‚Äëscores center variables at their means.

1) (0, 0) ‚Äî intersection of mean lines  
"1" = "‚úÖ Correct! In z‚Äëscores, the bivariate center is (0,0)."
2) (1, 1) ‚Äî one standard deviation point  
"2" = "‚ùå Incorrect. That's one SD above both means, not the center."
3) The densest part of the cloud  
"3" = "‚ùå Incorrect. The mode isn't necessarily the mean‚Äëcenter."
4) The most frequent observed point  
"4" = "‚ùå Incorrect. That's the mode, not the mean‚Äëbased center in z‚Äëspace."

---

### Question Q11 (Understand)
**What is the main lesson of Anscombe's Quartet?**

> **Hint:** Four datasets can share identical summaries yet have very different shapes.

1) Correlation tells the whole story  
"1" = "‚ùå Incorrect. Identical r can arise from very different patterns."
2) Always plot your data first  
"2" = "‚úÖ Correct! Visualization avoids being misled by summaries alone."
3) Graphs are less reliable than statistics  
"3" = "‚ùå Incorrect. Plots complement statistics; both matter."
4) Linear relationships are the most common  
"4" = "‚ùå Incorrect. The quartet shows diverse structures with the same r."

---

### Question Q12 (Analyze)
**Headline: "Ice cream sales and drownings are strongly correlated." What's a plausible explanation?**

> **Hint:** Think of a third variable (confounder) driving both.

1) Ice cream causes drownings  
"1" = "‚ùå Incorrect. Correlation ‚â† causation."
2) Drownings cause higher ice cream sales  
"2" = "‚ùå Incorrect. Reverse causality isn't plausible here."
3) Temperature/season increases both  
"3" = "‚úÖ Correct! Warm weather raises ice cream sales and swimming activity."
4) There is no relationship at all  
"4" = "‚ùå Incorrect. There is an association, just not causal."

---

### Question Q13 (Apply)
**The relationship is curved but monotonically increasing. Which correlation should you choose?**

> **Hint:** Distinguish "linear" from "monotone".

1) Pearson correlation  
"1" = "‚ùå Incorrect. Pearson measures linear association and may underestimate strength."
2) Spearman correlation  
"2" = "‚úÖ Correct! Spearman captures monotone relationships via ranks."
3) Both are equally appropriate  
"3" = "‚ùå Incorrect. Spearman fits better here."
4) Neither  
"4" = "‚ùå Incorrect. Spearman is suitable for monotone, non‚Äëlinear patterns."

---

### Question Q14 (Remember)
**You switch X from meters to centimeters. What happens to r(X,Y)?**

> **Hint:** Correlation is scale‚Äëinvariant; covariance isn't.

1) r becomes 100√ó larger  
"1" = "‚ùå Incorrect. Units don't affect r."
2) r becomes 100√ó smaller  
"2" = "‚ùå Incorrect. Units don't affect r."
3) r stays exactly the same  
"3" = "‚úÖ Correct! r is unit‚Äëfree and invariant to linear rescaling."
4) r becomes negative  
"4" = "‚ùå Incorrect. The sign doesn't flip due to units."

---

### Question Q15 (Understand)
**Why do z‚Äëscores make axes more comparable in a scatterplot?**

> **Hint:** Consider "same scale" and "standard deviations".

1) They put both variables into SD units  
"1" = "‚úÖ Correct! Standardizing places both on the same scale."
2) They always increase the correlation  
"2" = "‚ùå Incorrect. Standardizing doesn't change r."
3) They remove all outliers  
"3" = "‚ùå Incorrect. They may help detect outliers, not remove them."
4) They make covariance unit‚Äëfree  
"4" = "‚ùå Incorrect. Correlation is unit‚Äëfree; covariance remains scale‚Äëdependent."

---

### Question Q16 (Analyze)
**A scatterplot colors points by 'neighborhood type'. How is the overall correlation r typically computed?**

> **Hint:** Compare subgroup calculations versus full sample.

1) Using only the largest subgroup  
"1" = "‚ùå Incorrect. That ignores many cases."
2) As the average of subgroup correlations  
"2" = "‚ùå Incorrect. A simple mean of subgroup r's isn't standard."
3) Using all points together, ignoring colors  
"3" = "‚úÖ Correct! By default r is computed on the full dataset."
4) As a weighted average of subgroup r's  
"4" = "‚ùå Incorrect. Not the default definition of r."

---

### Question Q17 (Understand)
**Which quadrants dominate under a positive association (in z‚Äëscores)?**

> **Hint:** Consider the signs of z(X) and z(Y).

1) Mostly quadrants I & III  
"1" = "‚úÖ Correct! Positive: both above or both below their means."
2) Mostly quadrants II & IV  
"2" = "‚ùå Incorrect. That indicates negative association."
3) Evenly spread across all quadrants  
"3" = "‚ùå Incorrect. Suggests little/no linear relationship."
4) Only quadrant I  
"4" = "‚ùå Incorrect. Quadrant III also occurs in positive association."

---

### Question Q18 (Apply)
**A straight trend line slopes upward; points cluster tightly. Which description fits best?**

> **Hint:** Combine direction (sign) with strength (spread).

1) Strong positive  
"1" = "‚úÖ Correct! Upward line + tight cluster ‚Üí strong positive."
2) Moderate positive  
"2" = "‚ùå Incorrect. Description suggests stronger than moderate."
3) Weak negative  
"3" = "‚ùå Incorrect. Direction is positive, not negative."
4) No linear pattern  
"4" = "‚ùå Incorrect. The pattern is clearly linear."

---

### Question Q19 (Analyze)
**In a positive trend, which outlier position most reduces r?**

> **Hint:** Think of points that break the linear pattern.

1) Far bottom‚Äëleft (low X, very low Y)  
"1" = "‚ùå Incorrect. That aligns with the positive trend."
2) Far bottom‚Äëright (high X, low Y)  
"2" = "‚úÖ Correct! Pulls the cloud off the line and lowers r."
3) Far top‚Äëright (high X, high Y)  
"3" = "‚ùå Incorrect. Often increases a positive r."
4) A point exactly on the line  
"4" = "‚ùå Incorrect. Minimal impact on r."

---

### Question Q20 (Understand)
**Which statement about covariance and correlation is true?**

> **Hint:** Watch for units and ranges.

1) Covariance is unit‚Äëfree and bounded ‚àí1 to +1  
"1" = "‚ùå Incorrect. That's correlation, not covariance."
2) Correlation is unit‚Äëfree and always between ‚àí1 and +1  
"2" = "‚úÖ Correct! That's why r is comparable across variables."
3) Both are unbounded  
"3" = "‚ùå Incorrect. Only covariance is unbounded."
4) Both depend on measurement scale  
"4" = "‚ùå Incorrect. Only covariance is scale‚Äëdependent."

---

### Question Q21 (Apply)
**Which r value best matches 'weak positive'?**

> **Hint:** Use common interpretation guidelines.

1) r = 0.12  
"1" = "‚úÖ Correct! Often interpreted as weak positive."
2) r = 0.56  
"2" = "‚ùå Incorrect. Closer to moderate."
3) r = ‚àí0.72  
"3" = "‚ùå Incorrect. Strong negative, not weak positive."
4) r = 0.93  
"4" = "‚ùå Incorrect. Very strong positive."

---

### Question Q22 (Understand)
**"As X increases, Y tends to ‚Ä¶" ‚Äî choose the best completion for a slightly increasing pattern.**

> **Hint:** Use the standard sentence for a weak upward trend.

1) ‚Ä¶ decrease, strongly  
"1" = "‚ùå Incorrect. Wrong direction."
2) ‚Ä¶ increase, weakly  
"2" = "‚úÖ Correct! Weakly increasing relationship."
3) ‚Ä¶ increase, very strongly  
"3" = "‚ùå Incorrect. 'Very strong' doesn't fit a slight increase."
4) ‚Ä¶ stay the same  
"4" = "‚ùå Incorrect. That implies no linear relationship."

---

### Question Q23 (Remember)
**What does z = +1.2 mean in plain words?**

> **Hint:** Relate to the mean and standard deviation.

1) 1.2 units above the mean  
"1" = "‚ùå Incorrect. They are SD units, not raw units."
2) 1.2 standard deviations above the mean  
"2" = "‚úÖ Correct! That's the definition of a positive z."
3) 1.2% above the mean  
"3" = "‚ùå Incorrect. Not a percentage."
4) 0.12 standard deviations above the mean  
"4" = "‚ùå Incorrect. Wrong magnitude."

---

### Question Q24 (Analyze)
**Subgroup colors on the plot: what's a risk of looking only at overall r?**

> **Hint:** Think Simpson's paradox and heterogeneity across groups.

1) Subgroup patterns can mask or reverse the overall pattern  
"1" = "‚úÖ Correct! Aggregation can mislead; inspect subgroups."
2) r always changes when you add colors  
"2" = "‚ùå Incorrect. r's calculation ignores colors."
3) r can never be misleading  
"3" = "‚ùå Incorrect. r without context can mislead."
4) Subgroups automatically increase r  
"4" = "‚ùå Incorrect. No such general rule."

---

# Module 2: Correlation Calculations (18 questions)

### Question Q25 (Apply)
**Given X = [4, 6, 8] and Y = [2, 5, 8], calculate the means.**

> **Hint:** Sum divided by n.

1) XÃÑ = 5, »≤ = 4  
"1" = "‚ùå Incorrect. Check your calculations."
2) XÃÑ = 6, »≤ = 5  
"2" = "‚úÖ Correct! XÃÑ = (4+6+8)/3 = 6, »≤ = (2+5+8)/3 = 5."
3) XÃÑ = 18, »≤ = 15  
"3" = "‚ùå Incorrect. Don't forget to divide by n."
4) XÃÑ = 7, »≤ = 6  
"4" = "‚ùå Incorrect. Recalculate."

---

### Question Q26 (Apply)
**With XÃÑ = 6 and »≤ = 5, calculate the deviations for X = [4, 6, 8].**

> **Hint:** Deviation = Xi - XÃÑ

1) [-2, 0, 2]  
"1" = "‚úÖ Correct! (4-6), (6-6), (8-6) = [-2, 0, 2]."
2) [2, 0, -2]  
"2" = "‚ùå Incorrect. Signs are wrong."
3) [-1, 1, 1]  
"3" = "‚ùå Incorrect. Wrong calculations."
4) [4, 6, 8]  
"4" = "‚ùå Incorrect. These are the original values, not deviations."

---

### Question Q27 (Apply)
**Calculate the sum of squared deviations for X with deviations [-2, 0, 2].**

> **Hint:** Sum of (deviation)¬≤.

1) 4  
"1" = "‚ùå Incorrect. That's just one squared deviation."
2) 8  
"2" = "‚úÖ Correct! (-2)¬≤ + 0¬≤ + 2¬≤ = 4 + 0 + 4 = 8."
3) 0  
"3" = "‚ùå Incorrect. Sum of deviations is 0, but sum of squared deviations isn't."
4) 16  
"4" = "‚ùå Incorrect. Check your calculation."

---

### Question Q38 (Apply)
**With sum of squared deviations = 8 and n = 3, calculate the sample variance (s¬≤).**

> **Hint:** s¬≤ = SS/(n-1)

1) 2.67  
"1" = "‚ùå Incorrect. You used n instead of n-1."
2) 4  
"2" = "‚úÖ Correct! s¬≤ = 8/(3-1) = 8/2 = 4."
3) 8  
"3" = "‚ùå Incorrect. Don't forget to divide."
4) 1.33  
"4" = "‚ùå Incorrect. Wrong calculation."

---

### Question Q39 (Apply)
**Calculate the covariance with deviations X: [-2, 0, 2] and Y: [-3, 0, 3].**

> **Hint:** Cov = Œ£(Xi - XÃÑ)(Yi - »≤)/(n-1)

1) 6  
"1" = "‚úÖ Correct! [(-2)(-3) + (0)(0) + (2)(3)]/(3-1) = (6+0+6)/2 = 6."
2) 12  
"2" = "‚ùå Incorrect. Don't forget to divide by n-1."
3) 3  
"3" = "‚ùå Incorrect. Wrong calculation."
4) 0  
"4" = "‚ùå Incorrect. The products don't sum to zero."

---

### Question Q40 (Apply)
**With Cov(X,Y) = 6, sX = 2, sY = 1.73, calculate Pearson r.**

> **Hint:** r = Cov(X,Y)/(sX √ó sY)

1) 1.73  
"1" = "‚ùå Incorrect. r must be between -1 and +1."
2) 0.87  
"2" = "‚úÖ Correct! r = 6/(2 √ó 1.73) = 6/3.46 ‚âà 0.87."
3) 3.46  
"3" = "‚ùå Incorrect. That's the denominator."
4) 0.58  
"4" = "‚ùå Incorrect. Wrong calculation."

---

### Question Q41 (Understand)
**If all data points fall exactly on a straight line with positive slope, r equals:**

> **Hint:** Perfect linear relationship.

1) The slope of the line  
"1" = "‚ùå Incorrect. r is scale-free; slope depends on units."
2) +1  
"2" = "‚úÖ Correct! Perfect positive linear relationship gives r = +1."
3) The intercept of the line  
"3" = "‚ùå Incorrect. Intercept is different from correlation."
4) 0  
"4" = "‚ùå Incorrect. r = 0 indicates no linear relationship."

---

### Question Q42 (Analyze)
**Which r value indicates the strongest relationship (regardless of direction)?**

> **Hint:** Strength is about absolute value.

1) r = 0.85  
"1" = "‚ùå Incorrect. Not the strongest absolute value."
2) r = -0.92  
"2" = "‚úÖ Correct! |-0.92| = 0.92 is the highest absolute value."
3) r = 0.78  
"3" = "‚ùå Incorrect. Lower absolute value."
4) r = -0.15  
"4" = "‚ùå Incorrect. Weakest relationship."

---

### Question Q43 (Apply)
**If r = 0.87, what is R¬≤ (coefficient of determination)?**

> **Hint:** R¬≤ = r¬≤

1) 0.87  
"1" = "‚ùå Incorrect. That's r, not r¬≤."
2) 0.76  
"2" = "‚úÖ Correct! R¬≤ = (0.87)¬≤ = 0.7569 ‚âà 0.76."
3) 1.74  
"3" = "‚ùå Incorrect. Can't be greater than 1."
4) 0.93  
"4" = "‚ùå Incorrect. Wrong calculation."

---

### Question Q44 (Apply)
**Convert to ranks: X = [85, 92, 78, 92]. What are the ranks for X?**

> **Hint:** Assign ranks 1-4, handle ties with averages.

1) [2, 3.5, 1, 3.5]  
"1" = "‚úÖ Correct! 78(1st), 85(2nd), 92s tied for 3rd&4th = 3.5 each."
2) [2, 4, 1, 3]  
"2" = "‚ùå Incorrect. Need to average tied ranks."
3) [3, 4, 1, 2]  
"3" = "‚ùå Incorrect. Wrong ranking."
4) [1, 2, 3, 4]  
"4" = "‚ùå Incorrect. Ignores the tie."

---

### Question Q45 (Understand)

> **Hint:** Preserve the total sum of ranks.

1) To make calculations easier  
"1" = "‚ùå Incorrect. Not about convenience."
2) To maintain the sum of ranks = n(n+1)/2  
"2" = "‚úÖ Correct! Averaging preserves the mathematical properties."
3) Ties are not allowed in statistics  
"3" = "‚ùå Incorrect. Ties are common and handled systematically."
4) To increase the correlation  
"4" = "‚ùå Incorrect. Not about changing correlation strength."

---

### Question Q46 (Apply)
**For a 2√ó2 contingency table with œá¬≤ = 8.1 and n = 100, calculate Cram√©r's V.**

> **Hint:** V = ‚àö(œá¬≤/(n √ó min(r-1, c-1)))

1) 0.081  
"1" = "‚ùå Incorrect. Wrong formula application."
2) 0.285  
"2" = "‚úÖ Correct! V = ‚àö(8.1/(100√ó1)) = ‚àö0.081 = 0.285."
3) 0.81  
"3" = "‚ùå Incorrect. Missing the square root."
4) 0.09  
"4" = "‚ùå Incorrect. Wrong calculation."

---

### Question Q47 (Apply)

> **Hint:** Use regular Pearson formula or special point-biserial formula.

1) 0.82  
"1" = "‚úÖ Correct! Strong positive association between binary and continuous variables."
2) 0.00  
"2" = "‚ùå Incorrect. There is a clear relationship."
3) -0.82  
"3" = "‚ùå Incorrect. Relationship is positive."
4) 1.00  
"4" = "‚ùå Incorrect. Not a perfect relationship."

---

### Question Q48 (Analyze)
**Your data has extreme outliers and curved relationships. Which correlation measure is most appropriate?**

> **Hint:** Consider robustness to outliers and non-linearity.

1) Pearson correlation  
"1" = "‚ùå Incorrect. Sensitive to outliers and assumes linearity."
2) Spearman correlation  
"2" = "‚úÖ Correct! Rank-based, robust to outliers and captures monotonic relationships."
3) Point-biserial  
"3" = "‚ùå Incorrect. Only for binary-continuous relationships."
4) Cram√©r's V  
"4" = "‚ùå Incorrect. For categorical variables."

---

### Question Q49 (Analyze)
**One extreme outlier pulls the correlation from r = 0.85 to r = 0.23. What does this suggest?**

> **Hint:** Consider the impact of influential points.

1) The original correlation was wrong  
"1" = "‚ùå Incorrect. Both can be 'correct' calculations."
2) Outliers can dramatically affect Pearson r  
"2" = "‚úÖ Correct! Demonstrates sensitivity to extreme values."
3) The relationship is actually negative  
"3" = "‚ùå Incorrect. Both correlations are positive."
4) Sample size is too small  
"4" = "‚ùå Incorrect. Issue is about outlier influence, not sample size."

---

### Question Q50 (Evaluate)
**When should you prefer Spearman over Pearson correlation?**

> **Hint:** Consider data type and relationship characteristics.

1) When you want the strongest correlation  
"1" = "‚ùå Incorrect. Choice should be based on data characteristics, not strength."
2) When data is ordinal or relationship is monotonic but non-linear  
"2" = "‚úÖ Correct! Spearman is appropriate for ranks and monotonic relationships."
3) When sample size is large  
"3" = "‚ùå Incorrect. Sample size alone doesn't determine the choice."
4) When you have missing data  
"4" = "‚ùå Incorrect. Both require complete data for standard calculation."

---

### Question Q51 (Understand)

> **Hint:** Think about standardization.

1) They're the same thing  
"1" = "‚ùå Incorrect. Related but different."
2) Correlation is standardized covariance  
"2" = "‚úÖ Correct! r = Cov(X,Y)/(sX √ó sY)."
3) Covariance is always larger  
"3" = "‚ùå Incorrect. Depends on the scale."
4) They have opposite signs  
"4" = "‚ùå Incorrect. They have the same sign."

---

### Question Q52 (Apply)
**Interpret r = -0.65 between study hours and error rate.**

> **Hint:** Consider direction, strength, and practical meaning.

1) Moderate positive: more study, more errors  
"1" = "‚ùå Incorrect. r is negative."
2) Moderate negative: more study, fewer errors  
"2" = "‚úÖ Correct! Makes practical sense - studying reduces errors."
3) Weak relationship with no practical meaning  
"3" = "‚ùå Incorrect. -0.65 is moderate to strong."
4) Perfect negative relationship  
"4" = "‚ùå Incorrect. Perfect would be r = -1.00."

---

# Module 3a: From Correlation to Regression (7 questions)

### Question Q53 (Understand)
**Looking at a scatterplot, which pattern suggests the strongest positive correlation?**

> **Hint:** Consider both direction and scatter around the trend.

1) Points scattered randomly with no clear pattern  
"1" = "‚ùå Incorrect. Random scatter indicates no correlation."
2) Points tightly clustered around an upward-sloping line  
"2" = "‚úÖ Correct! Tight clustering around positive trend = strong positive correlation."
3) Points forming a perfect horizontal line  
"3" = "‚ùå Incorrect. Horizontal indicates no relationship between variables."
4) Points loosely scattered around a downward trend  
"4" = "‚ùå Incorrect. This suggests weak negative correlation."

---

### Question Q54 (Apply)
**If r = 0.9 between study hours and exam scores, what can we predict?**

> **Hint:** Strong correlation enables better prediction.

1) Students who study more will definitely get higher scores  
"1" = "‚ùå Incorrect. Correlation allows prediction but not certainty."
2) Study hours have a strong positive association with exam scores, enabling good predictions  
"2" = "‚úÖ Correct! High correlation means we can make reliable (but not perfect) predictions."
3) Study hours cause exam scores to increase  
"3" = "‚ùå Incorrect. Correlation doesn't establish causation."
4) There is no relationship between study hours and scores  
"4" = "‚ùå Incorrect. r = 0.9 indicates a very strong relationship."

---

### Question Q55 (Apply)
**From correlation to regression line: If r = 0.8, what does this tell us about the regression slope?**

> **Hint:** Think about the formula connecting r and b.

1) The slope will be positive and moderately steep  
"1" = "‚úÖ Correct! Positive r leads to positive slope; 0.8 is strong correlation."
2) The slope will be exactly 0.8  
"2" = "‚ùå Incorrect. Slope depends on both r and the ratio of standard deviations."
3) The slope will be negative  
"3" = "‚ùå Incorrect. Positive correlation means positive slope."
4) The slope cannot be determined from r alone  
"4" = "‚ùå Incorrect. r's sign always determines slope's sign."

---

### Question Q56 (Analyze)
**If R¬≤ = 0.64, how much prediction error remains unexplained?**

> **Hint:** R¬≤ is the explained proportion; what's left?

1) 64% remains unexplained  
"1" = "‚ùå Incorrect. R¬≤ represents explained variance."
2) 36% remains unexplained  
"2" = "‚úÖ Correct! If 64% is explained, then 100% - 64% = 36% is unexplained."
3) 0.64 units remain unexplained  
"3" = "‚ùå Incorrect. R¬≤ is a proportion, not a raw amount."
4) No error remains ‚Äî perfect prediction  
"4" = "‚ùå Incorrect. R¬≤ = 1.0 would indicate perfect prediction."

---

### Question Q57 (Apply)
**A regression gives ≈∂ = 50 + 3X. If X increases by 2 units, Y increases by:**

> **Hint:** Use the slope to calculate the change.

1) 2 units  
"1" = "‚ùå Incorrect. The slope is 3, not 1."
2) 3 units  
"2" = "‚ùå Incorrect. That's for a 1-unit change in X."
3) 6 units  
"3" = "‚úÖ Correct! Slope √ó change in X = 3 √ó 2 = 6 units."
4) 50 units  
"4" = "‚ùå Incorrect. That's the intercept, not the change."

---

### Question Q58 (Understand)
**What do residuals in a "good" regression model look like?**

> **Hint:** Think about patterns that would violate regression assumptions.

1) Large positive residuals at high X values  
"1" = "‚ùå Incorrect. This suggests systematic bias."
2) Randomly scattered around zero with no clear pattern  
"2" = "‚úÖ Correct! Random residuals indicate the model captures the systematic relationship well."
3) Residuals that increase steadily as X increases  
"3" = "‚ùå Incorrect. This indicates heteroscedasticity (assumption violation)."
4) All residuals exactly equal to zero  
"4" = "‚ùå Incorrect. This would require perfect prediction (unlikely with real data)."

---

### Question Q59 (Evaluate)
**When is it appropriate to use regression for prediction?**

> **Hint:** Consider the conditions that make regression reliable.

1) Always, regardless of the correlation strength  
"1" = "‚ùå Incorrect. Weak correlations lead to poor predictions."
2) When there's a strong linear relationship and you're predicting within the data range  
"2" = "‚úÖ Correct! Strong relationships and avoiding extrapolation are key conditions."
3) Only when you can prove causation  
"3" = "‚ùå Incorrect. Prediction doesn't require proving causation."
4) Only with experimental data  
"4" = "‚ùå Incorrect. Regression works with observational data too."

---

# Module 3: Regression Basics (10 questions)

### Question Q60 (Understand)
**What does the "least squares" principle minimize in regression?**

> **Hint:** Think about what OLS stands for.

1) The sum of errors (residuals)  
"1" = "‚ùå Incorrect. Sum of residuals is always zero."
2) The sum of absolute errors  
"2" = "‚ùå Incorrect. That's least absolute deviations."
3) The sum of squared errors  
"3" = "‚úÖ Correct! OLS minimizes Œ£(yi - ≈∑i)¬≤."
4) The correlation coefficient  
"4" = "‚ùå Incorrect. OLS doesn't minimize correlation."

---

### Question Q44 (Remember)
**Where does the regression line always pass through?**

> **Hint:** A fundamental property of OLS regression.

1) The origin (0,0)  
"1" = "‚ùå Incorrect. Only if both means are zero."
2) The point of means (XÃÑ, »≤)  
"2" = "‚úÖ Correct! The line always goes through (XÃÑ, »≤)."
3) The highest data point  
"3" = "‚ùå Incorrect. Not a property of regression lines."
4) The lowest data point  
"4" = "‚ùå Incorrect. Not a property of regression lines."

---

### Question Q45 (Understand)
**If the regression equation is ≈∂ = 5 + 2X, how do you interpret the slope?**

> **Hint:** Slope indicates the change in Y per unit change in X.

1) Y increases by 5 units for each unit increase in X  
"1" = "‚ùå Incorrect. That's the intercept, not slope."
2) Y increases by 2 units for each unit increase in X  
"2" = "‚úÖ Correct! Slope = 2 means ŒîY = 2 when ŒîX = 1."
3) Y increases by 7 units for each unit increase in X  
"3" = "‚ùå Incorrect. That's adding intercept and slope."
4) X increases by 2 units for each unit increase in Y  
"4" = "‚ùå Incorrect. Backwards interpretation."

---

### Question Q46 (Understand)
**In ≈∂ = 5 + 2X, what does the intercept (5) represent?**

> **Hint:** Value of Y when X = 0.

1) The slope of the line  
"1" = "‚ùå Incorrect. Slope is 2."
2) The predicted Y when X = 0  
"2" = "‚úÖ Correct! Intercept is the Y-value when X = 0."
3) The correlation between X and Y  
"3" = "‚ùå Incorrect. Correlation is different from intercept."
4) The average value of Y  
"4" = "‚ùå Incorrect. That's »≤, not the intercept."

---

### Question Q47 (Understand)
**What is a residual in regression analysis?**

> **Hint:** Think about prediction error.

1) The slope of the regression line  
"1" = "‚ùå Incorrect. That's a regression coefficient."
2) The difference between observed and predicted Y  
"2" = "‚úÖ Correct! Residual = yi - ≈∑i."
3) The correlation coefficient  
"3" = "‚ùå Incorrect. That measures linear association."
4) The intercept of the regression line  
"4" = "‚ùå Incorrect. That's a regression coefficient."

---

### Question Q48 (Understand)
**Why does OLS use squared errors rather than absolute errors?**

> **Hint:** Think about mathematical properties and outlier sensitivity.

1) Squared errors are easier to interpret  
"1" = "‚ùå Incorrect. Actually less intuitive than absolute errors."
2) It gives more weight to larger errors and has nice calculus properties  
"2" = "‚úÖ Correct! Penalizes large errors more and allows analytical solutions."
3) Absolute errors don't exist  
"3" = "‚ùå Incorrect. They exist but have different properties."
4) Squared errors are always smaller  
"4" = "‚ùå Incorrect. Only when |error| < 1."

---

### Question Q49 (Understand)
**What does R¬≤ = 0.64 mean in regression?**

> **Hint:** Proportion of variance explained.

1) 64% of Y's variance is explained by X  
"1" = "‚úÖ Correct! R¬≤ represents explained variance."
2) The correlation is 0.64  
"2" = "‚ùå Incorrect. r = ‚àö0.64 = 0.80."
3) 64% of predictions are correct  
"3" = "‚ùå Incorrect. R¬≤ isn't about prediction accuracy percentage."
4) The slope is 0.64  
"4" = "‚ùå Incorrect. R¬≤ and slope are different concepts."

---

### Question Q50 (Analyze)
**What's the main risk of extrapolation in regression?**

> **Hint:** Using the model outside the data range.

1) Calculations become more difficult  
"1" = "‚ùå Incorrect. Calculations are equally easy."
2) The relationship may not hold outside the observed range  
"2" = "‚úÖ Correct! Linear relationship might break down beyond data boundaries."
3) R¬≤ always decreases  
"3" = "‚ùå Incorrect. R¬≤ is calculated on existing data."
4) The slope changes  
"4" = "‚ùå Incorrect. The equation stays the same; validity is questioned."

---

### Question Q51 (Understand)
**How do correlation and regression differ?**

> **Hint:** Think about symmetry and prediction.

1) They're identical concepts  
"1" = "‚ùå Incorrect. Related but different."
2) Correlation is symmetric; regression has directional prediction  
"2" = "‚úÖ Correct! r(X,Y) = r(Y,X), but regression of Y on X ‚â† X on Y."
3) Correlation is more accurate  
"3" = "‚ùå Incorrect. They serve different purposes."
4) Regression doesn't use correlation  
"4" = "‚ùå Incorrect. They're closely related."

---

### Question Q52 (Analyze)
**You find a strong regression relationship (R¬≤ = 0.81). Can you conclude causation?**

> **Hint:** Think about confounding and experimental design.

1) Yes, high R¬≤ proves causation  
"1" = "‚ùå Incorrect. Association ‚â† causation, regardless of strength."
2) No, need experimental or quasi-experimental design  
"2" = "‚úÖ Correct! Correlation/regression alone cannot establish causation."
3) Only if the slope is positive  
"3" = "‚ùå Incorrect. Direction doesn't determine causation."
4) Yes, but only for R¬≤ > 0.80  
"4" = "‚ùå Incorrect. No threshold makes correlation become causation."

---

# Module 4: Regression Calculations (12 questions)

### Question Q53 (Apply)
**Calculate the slope using b = r(sy/sx) where r = 0.6, sy = 8, sx = 4.**

> **Hint:** Direct substitution into the formula.

1) 0.3  
"1" = "‚ùå Incorrect. Wrong calculation."
2) 1.2  
"2" = "‚úÖ Correct! b = 0.6 √ó (8/4) = 0.6 √ó 2 = 1.2."
3) 2.4  
"3" = "‚ùå Incorrect. Check your arithmetic."
4) 0.6  
"4" = "‚ùå Incorrect. That's just r."

---

### Question Q54 (Apply)
**Calculate the intercept using a = »≤ - bXÃÑ where »≤ = 12, b = 1.2, XÃÑ = 5.**

> **Hint:** Direct substitution into the formula.

1) 6.0  
"1" = "‚úÖ Correct! a = 12 - 1.2(5) = 12 - 6 = 6."
2) 18.0  
"2" = "‚ùå Incorrect. Check your signs."
3) 1.2  
"3" = "‚ùå Incorrect. That's the slope."
4) 12.0  
"4" = "‚ùå Incorrect. That's just »≤."

---

### Question Q55 (Apply)
**Using ≈∂ = 6 + 1.2X, predict Y when X = 8.**

> **Hint:** Substitute X = 8 into the equation.

1) 15.6  
"1" = "‚úÖ Correct! ≈∂ = 6 + 1.2(8) = 6 + 9.6 = 15.6."
2) 9.6  
"2" = "‚ùå Incorrect. Don't forget the intercept."
3) 20.4  
"3" = "‚ùå Incorrect. Check your calculation."
4) 6.0  
"4" = "‚ùå Incorrect. That's just the intercept."

---

### Question Q56 (Apply)
**If observed Y = 14 and predicted ≈∂ = 15.6, what's the residual?**

> **Hint:** Residual = observed - predicted.

1) 1.6  
"1" = "‚ùå Incorrect. Wrong sign."
2) -1.6  
"2" = "‚úÖ Correct! Residual = 14 - 15.6 = -1.6."
3) 29.6  
"3" = "‚ùå Incorrect. Don't add them."
4) 15.6  
"4" = "‚ùå Incorrect. That's the predicted value."

---

### Question Q57 (Apply)
**If r = 0.8, what is R¬≤?**

> **Hint:** R¬≤ = r¬≤

1) 0.8  
"1" = "‚ùå Incorrect. That's r, not r¬≤."
2) 0.64  
"2" = "‚úÖ Correct! R¬≤ = (0.8)¬≤ = 0.64."
3) 1.6  
"3" = "‚ùå Incorrect. Can't exceed 1."
4) 0.89  
"4" = "‚ùå Incorrect. Wrong calculation."

---

### Question Q58 (Apply)
**What's the standardized slope (beta) when r = 0.75?**

> **Hint:** In simple regression, Œ≤ = r.

1) 0.56  
"1" = "‚ùå Incorrect. That would be r¬≤."
2) 0.75  
"2" = "‚úÖ Correct! In simple regression, the standardized slope equals r."
3) 1.33  
"3" = "‚ùå Incorrect. Not possible given r = 0.75."
4) 0.87  
"4" = "‚ùå Incorrect. Wrong value."

---

### Question Q59 (Understand)
**If you change X from meters to centimeters, what happens to the slope?**

> **Hint:** Consider unit effects on regression coefficients.

1) Slope stays the same  
"1" = "‚ùå Incorrect. Slope is not scale-invariant."
2) Slope becomes 100 times smaller  
"2" = "‚úÖ Correct! When X units get 100√ó smaller, slope gets 100√ó smaller."
3) Slope becomes 100 times larger  
"3" = "‚ùå Incorrect. Wrong direction."
4) Slope becomes negative  
"4" = "‚ùå Incorrect. Sign doesn't change due to scale."

---

### Question Q60 (Apply)
**From regression output: Coefficient = 2.5, SE = 0.8, t = 3.125. What's the interpretation?**

> **Hint:** Focus on the coefficient and its meaning.

1) Y increases by 0.8 units per unit increase in X  
"1" = "‚ùå Incorrect. That's the standard error."
2) Y increases by 2.5 units per unit increase in X  
"2" = "‚úÖ Correct! The coefficient is the slope."
3) Y increases by 3.125 units per unit increase in X  
"3" = "‚ùå Incorrect. That's the t-statistic."
4) The relationship is not significant  
"4" = "‚ùå Incorrect. t = 3.125 suggests significance."

---

### Question Q61 (Analyze)
**An outlier pulls the slope from 2.1 to 3.8. What should you do?**

> **Hint:** Consider data investigation and robustness.

1) Always remove the outlier  
"1" = "‚ùå Incorrect. Need to investigate first."
2) Investigate the outlier and consider robust methods  
"2" = "‚úÖ Correct! Check if it's an error and consider robustness."
3) Always keep the outlier  
"3" = "‚ùå Incorrect. Depends on the situation."
4) Use the average of both slopes  
"4" = "‚ùå Incorrect. Not a standard practice."

---

### Question Q62 (Analyze)
**The intercept is -50 in a model predicting salary from years of experience. Is this meaningful?**

> **Hint:** Think about extrapolation and practical interpretation.

1) Yes, people with no experience owe money  
"1" = "‚ùå Incorrect. Practically nonsensical."
2) No, it's extrapolation beyond meaningful data range  
"2" = "‚úÖ Correct! Intercept may not be meaningful outside data range."
3) Yes, it's always meaningful  
"3" = "‚ùå Incorrect. Context matters."
4) No, intercepts are never meaningful  
"4" = "‚ùå Incorrect. Sometimes they are meaningful."

---

### Question Q63 (Understand)
**Which assumption is most important for least squares regression?**

> **Hint:** Think about core OLS assumptions.

1) X must be normally distributed  
"1" = "‚ùå Incorrect. X distribution isn't required to be normal."
2) Y must have constant variance across X values  
"2" = "‚úÖ Correct! Homoscedasticity is a key assumption."
3) Sample size must exceed 30  
"3" = "‚ùå Incorrect. No specific sample size requirement."
4) Variables must be measured without error  
"4" = "‚ùå Incorrect. While ideal, some measurement error is usually tolerated."

---

### Question Q64 (Understand)
**What's the difference between prediction and explanation in regression?**

> **Hint:** Think about goals and interpretation.

1) They're the same thing  
"1" = "‚ùå Incorrect. Different goals."
2) Prediction focuses on accuracy; explanation on understanding relationships  
"2" = "‚úÖ Correct! Different emphases and evaluation criteria."
3) Prediction is always better  
"3" = "‚ùå Incorrect. Depends on the research goal."
4) Explanation doesn't use regression  
"4" = "‚ùå Incorrect. Regression serves both purposes."

---

# Module 5: Partial Correlation (8 questions)

### Question Q65 (Understand)
**When is partial correlation most useful?**

> **Hint:** Think about confounding variables.

1) When you want the strongest correlation  
"1" = "‚ùå Incorrect. Strength isn't the goal."
2) When you want to control for a third variable  
"2" = "‚úÖ Correct! Partial correlation removes the effect of confounders."
3) When sample size is small  
"3" = "‚ùå Incorrect. Not specifically about sample size."
4) When variables are categorical  
"4" = "‚ùå Incorrect. Partial correlation is for continuous variables."

---

### Question Q66 (Remember)
**What's the formula for partial correlation rXY.Z?**

> **Hint:** Uses pairwise correlations and algebraic manipulation.

1) rXY √ó rXZ √ó rYZ  
"1" = "‚ùå Incorrect. Not a product."
2) (rXY - rXZ √ó rYZ) / ‚àö[(1-rXZ¬≤)(1-rYZ¬≤)]  
"2" = "‚úÖ Correct! Standard partial correlation formula."
3) rXY - rXZ - rYZ  
"3" = "‚ùå Incorrect. Simple subtraction doesn't work."
4) (rXY + rXZ + rYZ) / 3  
"4" = "‚ùå Incorrect. Not an average."

---

### Question Q67 (Apply)
**Calculate rXY.Z with rXY = 0.7, rXZ = 0.5, rYZ = 0.6.**

> **Hint:** Use the partial correlation formula.

1) 0.45  
"1" = "‚úÖ Correct! rXY.Z = (0.7 - 0.5√ó0.6)/‚àö[(1-0.5¬≤)(1-0.6¬≤)] = 0.4/‚àö[0.75√ó0.64] ‚âà 0.45."
2) 0.70  
"2" = "‚ùå Incorrect. That's just rXY."
3) 0.60  
"3" = "‚ùå Incorrect. Wrong calculation."
4) 0.30  
"4" = "‚ùå Incorrect. Check your arithmetic."

---

### Question Q68 (Analyze)
**rXY = 0.8, but rXY.Z = 0.2. What does this suggest?**

> **Hint:** Think about spurious correlation.

1) Z has no effect  
"1" = "‚ùå Incorrect. Z clearly has a major effect."
2) Much of the X-Y relationship was due to Z  
"2" = "‚úÖ Correct! Z was confounding the X-Y relationship."
3) There was a calculation error  
"3" = "‚ùå Incorrect. This pattern is possible and meaningful."
4) X and Y are not related  
"4" = "‚ùå Incorrect. They still have some relationship (0.2)."

---

### Question Q69 (Understand)
**What's the difference between controlling for Z and interacting with Z?**

> **Hint:** Think about constant vs. variable effects.

1) They're the same concept  
"1" = "‚ùå Incorrect. Different concepts."
2) Controlling assumes Z's effect is constant; interaction allows it to vary  
"2" = "‚úÖ Correct! Control removes Z's effect; interaction examines how Z modifies relationships."
3) Interaction is always better  
"3" = "‚ùå Incorrect. Depends on the research question."
4) Controlling is only for categorical variables  
"4" = "‚ùå Incorrect. Can control for any type of variable."

---

### Question Q70 (Analyze)
**Interpret rXY.Z = -0.3 when rXY = 0.1 originally.**

> **Hint:** Think about suppression effects.

1) Controlling for Z revealed a hidden negative relationship  
"1" = "‚úÖ Correct! Z was suppressing the true negative X-Y relationship."
2) Z caused the relationship to become negative  
"2" = "‚ùå Incorrect. Z revealed what was already there."
3) There's no relationship between X and Y  
"3" = "‚ùå Incorrect. There is a negative relationship."
4) The calculation must be wrong  
"4" = "‚ùå Incorrect. Suppression effects are real phenomena."

---

### Question Q71 (Apply)
**How should you report partial correlation results?**

> **Hint:** Think about what readers need to know.

1) Only report the partial correlation value  
"1" = "‚ùå Incorrect. Need more context."
2) Report both zero-order and partial correlations with controlled variables  
"2" = "‚úÖ Correct! Provides full picture of relationships."
3) Only report if the partial correlation is significant  
"3" = "‚ùå Incorrect. Non-significant results can be meaningful."
4) Report only the variables that were controlled  
"4" = "‚ùå Incorrect. Need the actual correlation values."

---

### Question Q72 (Understand)
**Does changing the scale of X affect the partial correlation rXY.Z?**

> **Hint:** Think about correlation's scale-invariance property.

1) Yes, partial correlations are scale-dependent  
"1" = "‚ùå Incorrect. Correlations are scale-invariant."
2) No, all correlations (including partial) are scale-invariant  
"2" = "‚úÖ Correct! Scale changes don't affect any correlation measures."
3) Only if Z is also rescaled  
"3" = "‚ùå Incorrect. Scale-invariance holds regardless."
4) Yes, but only for the controlled variable  
"4" = "‚ùå Incorrect. All correlations are scale-invariant."

---

## Module 5: Partial Correlation

### Question Q73 (Understand)
**When should you use partial correlation instead of simple correlation?**

> **Hint:** Think about when you need to control for the influence of other variables.

1) When you have more than two variables and want to isolate the relationship between two specific variables  
"1" = "‚úÖ Correct! Partial correlation controls for third variables."
2) When your correlation coefficient is too small  
"2" = "‚ùå Incorrect. Partial correlation is about controlling variables, not size."
3) When you have missing data  
"3" = "‚ùå Incorrect. This relates to data handling, not correlation type."
4) When variables are not normally distributed  
"4" = "‚ùå Incorrect. This relates to correlation assumptions, not when to use partial correlation."

---

### Question Q74 (Remember)
**What is the formula for partial correlation rXY.Z?**

> **Hint:** Think about how you remove the influence of Z from both X and Y.

1) rXY.Z = rXY - rXZ √ó rYZ  
"1" = "‚ùå Incorrect. Missing the denominator."
2) rXY.Z = (rXY - rXZ √ó rYZ) / ‚àö((1-r¬≤XZ)(1-r¬≤YZ))  
"2" = "‚úÖ Correct! This removes Z's influence from both X and Y."
3) rXY.Z = rXY / (rXZ √ó rYZ)  
"3" = "‚ùå Incorrect. This is not the correct formula."
4) rXY.Z = rXY + rXZ - rYZ  
"4" = "‚ùå Incorrect. This doesn't control for Z properly."

---

### Question Q75 (Apply)
**Calculate the partial correlation rXY.Z given: rXY = 0.60, rXZ = 0.40, rYZ = 0.30**

> **Hint:** Use the partial correlation formula step by step.

1) 0.47  
"1" = "‚úÖ Correct! rXY.Z = (0.60 - 0.40√ó0.30) / ‚àö((1-0.16)(1-0.09)) = 0.48/‚àö(0.84√ó0.91) ‚âà 0.47"
2) 0.60  
"2" = "‚ùå Incorrect. This ignores the control for Z."
3) 0.30  
"3" = "‚ùå Incorrect. Check your calculation."
4) 0.72  
"4" = "‚ùå Incorrect. This is larger than the original correlation."

---

### Question Q76 (Analyze)
**What's the difference between spurious correlation and suppression?**

> **Hint:** Think about whether the third variable inflates or deflates the relationship.

1) Spurious correlation is when a third variable creates a false relationship; suppression is when it hides a true relationship  
"1" = "‚úÖ Correct! Both involve third variables but in opposite ways."
2) They're the same thing  
"2" = "‚ùå Incorrect. They're opposite phenomena."
3) Spurious correlation involves measurement error; suppression involves sampling error  
"3" = "‚ùå Incorrect. Both relate to third variable effects."
4) Spurious correlation is stronger; suppression is weaker  
"4" = "‚ùå Incorrect. This describes magnitude, not the underlying mechanism."

---

### Question Q77 (Understand)
**When controlling for variable Z, what does it mean if rXY.Z > rXY?**

> **Hint:** Think about what happens when Z was suppressing the true relationship.

1) Z was creating a spurious correlation  
"1" = "‚ùå Incorrect. Spurious correlation would make the partial correlation smaller."
2) Z was suppressing the true relationship between X and Y  
"2" = "‚úÖ Correct! Removing Z's influence reveals a stronger relationship."
3) There's an error in calculation  
"3" = "‚ùå Incorrect. This is a valid statistical phenomenon."
4) X and Y are now confounded  
"4" = "‚ùå Incorrect. Controlling for Z reduces confounding."

---

### Question Q78 (Analyze)
**How should you interpret a partial correlation of rXY.Z = 0.45 when rXY = 0.60?**

> **Hint:** Compare the magnitudes and think about Z's role.

1) Z was irrelevant to the X-Y relationship  
"1" = "‚ùå Incorrect. If Z were irrelevant, the correlations would be similar."
2) Z partially explains the X-Y relationship  
"2" = "‚úÖ Correct! The relationship is weaker after controlling for Z."
3) Z completely explains the X-Y relationship  
"3" = "‚ùå Incorrect. There's still a substantial relationship (0.45)."
4) The analysis is invalid  
"4" = "‚ùå Incorrect. This is a normal result."

---

### Question Q79 (Apply)
**When reporting partial correlations in criminology research, you should:**

> **Hint:** Think about complete and transparent reporting practices.

1) Only report the partial correlation value  
"1" = "‚ùå Incorrect. Readers need the full context."
2) Report both simple and partial correlations, plus what was controlled  
"2" = "‚úÖ Correct! Provides full picture of relationships."
3) Only report if the partial correlation is significant  
"3" = "‚ùå Incorrect. Non-significant results can be meaningful."
4) Report only the variables that were controlled  
"4" = "‚ùå Incorrect. Need the actual correlation values."

---

### Question Q80 (Understand)
**Does changing the scale of X affect the partial correlation rXY.Z?**

> **Hint:** Think about correlation's scale-invariance property.

1) Yes, partial correlations are scale-dependent  
"1" = "‚ùå Incorrect. Correlations are scale-invariant."
2) No, all correlations (including partial) are scale-invariant  
"2" = "‚úÖ Correct! Scale changes don't affect any correlation measures."
3) Only if Z is also rescaled  
"3" = "‚ùå Incorrect. Scale-invariance holds regardless."
4) Yes, but only for the controlled variable  
"4" = "‚ùå Incorrect. All correlations are scale-invariant."

---

## Summary

This expanded version provides comprehensive coverage of correlation and regression concepts across 80 carefully structured questions spanning five modules:

1. **Module 1 (34 questions)**: Fundamental concepts including what correlation and regression are, variable types, basic interpretation, and foundation concepts before moving to standardization and scatterplot interpretation
2. **Module 2 (18 questions)**: Detailed calculation procedures for various correlation measures  
3. **Module 3a (7 questions)**: Bridge module connecting correlation concepts to regression
4. **Module 3 (10 questions)**: Core regression concepts and interpretation
5. **Module 4 (12 questions)**: Regression calculations and practical applications
6. **Module 5 (8 questions)**: Advanced topics in partial correlation and control variables

The first 8 questions now cover the fundamental concepts you requested:
- **Q1-Q2**: Basic definitions of correlation and regression
- **Q3**: Variable types suitable for correlation analysis
- **Q4**: Why correlation doesn't prove causation
- **Q5**: What z-scores are and their purpose
- **Q6**: How to interpret correlation values
- **Q7**: Key measures in correlation analysis
- **Q8**: Understanding relationship direction

Each question includes hints and detailed feedback to support learning across Bloom's taxonomy levels.